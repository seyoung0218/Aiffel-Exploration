{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exploration6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 작사가 인공지능 만들기\n",
        "\n",
        "# Step1. 데이터 불러오기"
      ],
      "metadata": {
        "id": "FEZ2DzEPs2li"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0D6D3b9sppz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb5e5756-f001-42c6-b0a6-cf355260891c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 크기: 187088\n",
            "Examples:\n",
            " [\"Let's stay together I, I'm I'm so in love with you\", 'Whatever you want to do', 'Is all right with me', 'Cause you make me feel so brand new', \"And I want to spend my life with you Let me say that since, baby, since we've been together\", 'Loving you forever', 'Is what I need', 'Let me, be the one you come running to', \"I'll never be untrue Oh baby\", \"Let's, let's stay together (gether)\", \"Lovin' you whether, whether\", 'Times are good or bad, happy or sad', 'Oh, oh, oh, oh, yeah', 'Whether times are good or bad, happy or sad Why, why some people break up', 'Then turn around and make up', \"I just can't see\", \"You'd never do that to me (would you, baby)\", 'Staying around you is all I see', \"(Here's what I want us do) Let's, we oughta stay together (gether)\", 'Loving you whether, whether']\n"
          ]
        }
      ],
      "source": [
        "# glob모듈을 이용하여 모든 txt파일을 읽어와서 raw_corpus리스트에 문장 단위로 저장하겠습니다.\n",
        "import glob\n",
        "import os, re \n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "txt_file_path = '/content/drive/MyDrive/익스플러레이션 데이터 - 아이펠 /lyrics/*'\n",
        "\n",
        "txt_list = glob.glob(txt_file_path)\n",
        "\n",
        "raw_corpus = []\n",
        "\n",
        "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
        "for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        raw = f.read().splitlines()\n",
        "        raw_corpus.extend(raw)\n",
        "\n",
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\\n\", raw_corpus[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step2. 데이터 전처리 및 정제\n",
        "문장을 토큰화(Tokenize)시키기전에 몇가지 문제가 되는 상황이 있습니다.\n",
        "1. 문장부호\n",
        "2. 대소문자\n",
        "3. 특수문자\n",
        "\n",
        "해당 문제들을 제거해주기위해 전처리함수를 정의합니다."
      ],
      "metadata": {
        "id": "DbgJJH3mvRXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip() # 1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2. 특수문자 양쪽에 공백을 넣고\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
        "    sentence = sentence.strip() # 5. 다시 양쪽 공백을 지웁니다\n",
        "    sentence = '<start> ' + sentence + ' <end>' # 6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "wZN8zM_mvTRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "공백이나 화자등의 우리가 원하지않는 문장들을 정제해봅시다. 그리고 전처리도 같이 진행해줍니다."
      ],
      "metadata": {
        "id": "sAiw_fUMzYtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = []\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "    if len(sentence) == 0: continue\n",
        "    if sentence[-1] == \":\": continue\n",
        "    \n",
        "    preprocessed_sentence = preprocess_sentence(sentence)\n",
        "    corpus.append(preprocessed_sentence)\n",
        "        \n",
        "# 정제된 결과를 10개만 확인해보죠\n",
        "corpus[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZwKXyY9zhOP",
        "outputId": "cdc898e9-a930-42cb-b68b-0ae9f5485b53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> let s stay together i , i m i m so in love with you <end>',\n",
              " '<start> whatever you want to do <end>',\n",
              " '<start> is all right with me <end>',\n",
              " '<start> cause you make me feel so brand new <end>',\n",
              " '<start> and i want to spend my life with you let me say that since , baby , since we ve been together <end>',\n",
              " '<start> loving you forever <end>',\n",
              " '<start> is what i need <end>',\n",
              " '<start> let me , be the one you come running to <end>',\n",
              " '<start> i ll never be untrue oh baby <end>',\n",
              " '<start> let s , let s stay together gether <end>']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step3. 토근화\n",
        "텐서플로우의 토큰화패키지는 정제된 데이터를 토큰화해주고, 단어사전을 만들어주며, 데이터를 숫자로 변환까지 한 방에 해줍니다. 이 과정을 **벡터화**라하며, 숫자로 변환된 데이터를 **텐서**라고 부릅니다."
      ],
      "metadata": {
        "id": "cM9UlcKY0K_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(corpus):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=13000,     # 13,000단어를 기억할 수 있는 tokenizer를 만들어줍니다.\n",
        "        filters=' ',         # 이미 문장을 정제했으니 filters가 필요없습니다.\n",
        "        oov_token=\"<unk>\"    # 13,000단어에 포함되지 못한 단어는 '<unk>'로 바꿔줍니다.\n",
        "    )\n",
        "    \n",
        "    tokenizer.fit_on_texts(corpus)    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
        "    \n",
        "    tensor = tokenizer.texts_to_sequences(corpus)    # 준비한tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
        "  \n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
        "    \n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNstMR8B0hu5",
        "outputId": "6803c791-8d3b-4c3b-83f3-08c58d19dc2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  2  63  16 ...   0   0   0]\n",
            " [  2 598   7 ...   0   0   0]\n",
            " [  2  26  24 ...   0   0   0]\n",
            " ...\n",
            " [  2  35 134 ...   0   0   0]\n",
            " [  2   5  61 ...   0   0   0]\n",
            " [  2 171   3 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7fc78032c2d0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "생성된 텐서 데이터를 3번째행, 10번째 열까지만 출력해보겠습니다."
      ],
      "metadata": {
        "id": "tE5QI1Jg2IAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor.shape)\n",
        "print('')\n",
        "print(tensor[:3, :10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVRvqDel2MV4",
        "outputId": "01420323-9077-4927-a5a6-ba24b1173104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(175749, 347)\n",
            "\n",
            "[[  2  63  16 222 283   5   4   5  22   5]\n",
            " [  2 598   7  62  10  47   3   0   0   0]\n",
            " [  2  26  24  84  31  12   3   0   0   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "텐서 데이터는 모두 정수로 이루어져있고, 이 숫자는 tokenizer에 구축된 단어 사전의 인덱스입니다. 단어 사전이 어떻게 구축되어있는지 아래와 같이 확인해봅시다."
      ],
      "metadata": {
        "id": "5mZQ4Wte2nKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in tokenizer.index_word:\n",
        "    print(idx, \":\", tokenizer.index_word[idx])\n",
        "\n",
        "    if idx >= 10: break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQa01eIc232g",
        "outputId": "11f42a58-6698-4be5-f4b0-513a13ab9e12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 : <unk>\n",
            "2 : <start>\n",
            "3 : <end>\n",
            "4 : ,\n",
            "5 : i\n",
            "6 : the\n",
            "7 : you\n",
            "8 : and\n",
            "9 : a\n",
            "10 : to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기서 start는 인덱스2에해당하고 end는 인덱스3에 해당하는것을 확인할 수 있습니다."
      ],
      "metadata": {
        "id": "9sEecsle3btl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "너무 긴문장은 다른 데이터들이 과도한 padding을 갖게 하므로제거해줍니다.\n",
        "토근의 개수가 15개를 넘어가는 문장을 데이터에서 제거하겠습니다."
      ],
      "metadata": {
        "id": "AmIgfrR0WPX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 15개가 넘어가는 문장인 행제거\n",
        "i = 0\n",
        "try:\n",
        "  while tensor.shape[0] !=  i:\n",
        "    if tensor[i,15] != 0:\n",
        "      tensor = np.delete(tensor, i, axis = 0)\n",
        "    else:\n",
        "      i += 1 \n",
        "except:\n",
        "  print(\"완료되었습니다.\")\n",
        "\n",
        "tensor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaK1e-0TCLz_",
        "outputId": "f0b0dc2d-42e3-4a2c-abfe-6a75ca25223a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(156013, 347)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 15번째열 넘어서부터는 전부 0이기때문에, 해당 열을 전부 제거 \n",
        "while tensor.shape[1] != 15:\n",
        "    tensor = np.delete(tensor, 15 , axis = 1)\n",
        "\n",
        "tensor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLYE2b4ziiqk",
        "outputId": "74230289-4c76-48dc-8352-d8df0fb03690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(156013, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 소스문장과 타겟문장 생성"
      ],
      "metadata": {
        "id": "y0XzHGcB3PMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_input = tensor[:, :-1]  \n",
        "tgt_input = tensor[:, 1:]    \n",
        "\n",
        "print(src_input[0])\n",
        "print(tgt_input[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guT9EfBs3SHg",
        "outputId": "17b173aa-1332-49ae-a3a7-1bb05a628522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  2 598   7  62  10  47   3   0   0   0   0   0   0   0]\n",
            "[598   7  62  10  47   3   0   0   0   0   0   0   0   0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "소스문장이 처음과 끝이 2와3으로 끝나고, 타겟문장은 3으로 끝나는것을 확인할 수 있습니다.\n",
        "# Step4. 데이터셋 객체 생성\n",
        "\n",
        "텐서플로우를 활용할때 **데이터셋객체**는 데이터 입력 파이프라인을 통한 속도개선 및 각종 편의기능을 제공합니다."
      ],
      "metadata": {
        "id": "fZGkpYe_3lJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(src_input)\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
        "\n",
        "VOCAB_SIZE = tokenizer.num_words + 1  # tokenizer가 구축한 단어사전 내 13,000개와, 여기 포함되지 않은 0:<pad>를 포함하여 13,001개\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)) # 준비한 데이터 소스로부터 데이터셋객체를 만듭니다\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dq3ONzyF41es",
        "outputId": "fac35593-4fbb-4220-c460-126a2ce519d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(256, 14), dtype=tf.int32, name=None), TensorSpec(shape=(256, 14), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding 계층 : 텐서의 인덱스값을 해당 인덱스번째의 워드 벡터로 바꿔줍니다. 이 워드벡터는 의미벡터공간에서 단어의 추상적표현으로 사용됩니다.\n",
        "# Step5. 네트워크 구현\n"
      ],
      "metadata": {
        "id": "uSrBJvOs8RlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 계층 구현\n",
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "embedding_size = 256 # 워드벡터의 차원수\n",
        "hidden_size = 1024\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
      ],
      "metadata": {
        "id": "SBPBQKYZ7rSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n",
        "for src_sample, tgt_sample in dataset.take(1): break\n",
        "\n",
        "model(src_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uz-y73EFYr3Y",
        "outputId": "1abb1fdf-fe7b-4406-c246-6f96a2dd39fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(256, 14, 13001), dtype=float32, numpy=\n",
              "array([[[ 2.31591639e-05, -3.00065120e-04,  1.75035035e-04, ...,\n",
              "          2.78022362e-05,  1.29447479e-04,  6.40035068e-05],\n",
              "        [ 1.11434056e-04, -5.91128133e-04,  1.58930023e-04, ...,\n",
              "          2.91204537e-06,  1.63713470e-04,  1.53851186e-04],\n",
              "        [ 8.78944484e-05, -7.86744116e-04,  2.33918516e-04, ...,\n",
              "          2.00838622e-04,  3.38028127e-04,  5.46143201e-05],\n",
              "        ...,\n",
              "        [ 4.77558438e-04, -2.10592421e-04, -2.07982594e-04, ...,\n",
              "          4.98414331e-04,  9.18051344e-04, -6.76701253e-04],\n",
              "        [ 4.98222944e-04, -1.02681428e-04, -3.00775515e-04, ...,\n",
              "          7.33476074e-04,  1.50192517e-03, -9.43650899e-04],\n",
              "        [ 5.28636621e-04, -1.35444971e-05, -3.85415682e-04, ...,\n",
              "          9.89436405e-04,  2.04909313e-03, -1.22547452e-03]],\n",
              "\n",
              "       [[ 2.31591639e-05, -3.00065120e-04,  1.75035035e-04, ...,\n",
              "          2.78022362e-05,  1.29447479e-04,  6.40035068e-05],\n",
              "        [ 3.74716692e-05, -4.59301751e-04,  2.41138259e-04, ...,\n",
              "          3.29017814e-04,  2.06192475e-04,  2.02246520e-04],\n",
              "        [ 1.72575183e-05, -3.90781672e-04,  1.88675578e-04, ...,\n",
              "          6.75859163e-04,  3.61818966e-04,  4.59146220e-04],\n",
              "        ...,\n",
              "        [ 5.02099458e-04,  4.93452011e-04,  1.91383690e-04, ...,\n",
              "          9.07559588e-04,  2.50409800e-03, -6.94657734e-04],\n",
              "        [ 5.61796071e-04,  5.35296975e-04,  3.47886780e-05, ...,\n",
              "          1.08076935e-03,  2.87293061e-03, -9.87372478e-04],\n",
              "        [ 6.21509098e-04,  5.64536138e-04, -1.09301764e-04, ...,\n",
              "          1.26527785e-03,  3.20398388e-03, -1.29347050e-03]],\n",
              "\n",
              "       [[ 2.31591639e-05, -3.00065120e-04,  1.75035035e-04, ...,\n",
              "          2.78022362e-05,  1.29447479e-04,  6.40035068e-05],\n",
              "        [ 2.46789394e-04, -3.50937422e-04,  2.13068488e-04, ...,\n",
              "          2.03514996e-04,  1.71999200e-04,  2.32865961e-04],\n",
              "        [ 5.81610890e-04, -1.60897442e-04,  5.63609792e-05, ...,\n",
              "          2.76398903e-04,  4.15347677e-05,  9.93316862e-05],\n",
              "        ...,\n",
              "        [ 1.04010839e-03,  4.82017873e-04, -5.89946634e-04, ...,\n",
              "          1.14863164e-04, -1.74108965e-04, -6.34752563e-04],\n",
              "        [ 9.29123547e-04,  3.56909615e-04, -5.13807405e-04, ...,\n",
              "          1.88200560e-04,  2.18795860e-04, -7.97302462e-04],\n",
              "        [ 8.47734977e-04,  2.71449477e-04, -5.06358978e-04, ...,\n",
              "          3.81752878e-04,  7.32222339e-04, -9.61964368e-04]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 2.31591639e-05, -3.00065120e-04,  1.75035035e-04, ...,\n",
              "          2.78022362e-05,  1.29447479e-04,  6.40035068e-05],\n",
              "        [ 8.55020407e-05, -2.36380583e-04,  2.19125621e-04, ...,\n",
              "          1.58653667e-04,  5.69400909e-05,  2.64457951e-04],\n",
              "        [ 2.13959109e-04, -2.95166392e-04,  8.71709781e-05, ...,\n",
              "          2.55238672e-04,  1.84867444e-04,  6.30146751e-05],\n",
              "        ...,\n",
              "        [ 7.92354811e-04, -1.01394464e-04, -5.75391110e-04, ...,\n",
              "          1.95309625e-03,  3.54320183e-03, -2.26571341e-03],\n",
              "        [ 8.66165210e-04, -2.93996454e-05, -6.38726517e-04, ...,\n",
              "          2.11929646e-03,  3.80457309e-03, -2.51398026e-03],\n",
              "        [ 9.39235731e-04,  3.83530496e-05, -6.94495044e-04, ...,\n",
              "          2.25339225e-03,  4.02885908e-03, -2.74176896e-03]],\n",
              "\n",
              "       [[ 2.31591639e-05, -3.00065120e-04,  1.75035035e-04, ...,\n",
              "          2.78022362e-05,  1.29447479e-04,  6.40035068e-05],\n",
              "        [ 6.15574463e-05, -5.98059152e-04,  1.03885213e-04, ...,\n",
              "         -6.14438541e-05,  1.47981860e-04,  2.48747645e-04],\n",
              "        [ 2.26851102e-04, -4.97860252e-04,  5.82902758e-05, ...,\n",
              "         -8.42353472e-07,  2.70111079e-04,  6.50383532e-04],\n",
              "        ...,\n",
              "        [ 2.16118482e-04,  3.64034277e-05, -3.42951767e-04, ...,\n",
              "          1.39088510e-03,  3.35277547e-03, -1.25155784e-03],\n",
              "        [ 3.20263091e-04,  1.07279506e-04, -4.40920790e-04, ...,\n",
              "          1.59801717e-03,  3.66888638e-03, -1.60049868e-03],\n",
              "        [ 4.33799840e-04,  1.71187072e-04, -5.29318291e-04, ...,\n",
              "          1.77906849e-03,  3.93593824e-03, -1.93114486e-03]],\n",
              "\n",
              "       [[ 2.31591639e-05, -3.00065120e-04,  1.75035035e-04, ...,\n",
              "          2.78022362e-05,  1.29447479e-04,  6.40035068e-05],\n",
              "        [ 2.54312501e-04, -3.23287299e-04, -6.68646462e-05, ...,\n",
              "         -2.15743239e-05, -1.96785157e-04,  1.71662177e-04],\n",
              "        [ 4.35343507e-04, -6.14581048e-04, -6.87541324e-05, ...,\n",
              "          2.07809513e-04, -6.58367528e-04,  1.86587466e-04],\n",
              "        ...,\n",
              "        [ 1.00241159e-03, -8.67612136e-04, -2.88539304e-04, ...,\n",
              "          6.73317234e-04,  1.42562587e-03, -1.66974845e-03],\n",
              "        [ 1.01390400e-03, -7.29414518e-04, -3.94176721e-04, ...,\n",
              "          9.06311383e-04,  2.02785432e-03, -1.96456583e-03],\n",
              "        [ 1.03415817e-03, -5.87603427e-04, -4.93499916e-04, ...,\n",
              "          1.12532941e-03,  2.55806209e-03, -2.24432908e-03]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDgS26wHYuZp",
        "outputId": "f7f5f326-5fb4-4b85-bfc6-5585e3f86c8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"text_generator_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     multiple                  3328256   \n",
            "                                                                 \n",
            " lstm_6 (LSTM)               multiple                  5246976   \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               multiple                  8392704   \n",
            "                                                                 \n",
            " dense_3 (Dense)             multiple                  13326025  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30,293,961\n",
            "Trainable params: 30,293,961\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "매개변수가 3천만개가 쓰였습니다."
      ],
      "metadata": {
        "id": "-0qObWtq5Gre"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step6. 모델 훈련시키기"
      ],
      "metadata": {
        "id": "q2XMIi0j1ncu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습시키기\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction='none'\n",
        ")\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "\n",
        "\n",
        "model.fit(dataset, epochs=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDrlNNAm_gKa",
        "outputId": "7f6ca0d7-4faa-4567-8639-1dab4bca073b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "609/609 [==============================] - 59s 94ms/step - loss: 3.4361\n",
            "Epoch 2/15\n",
            "609/609 [==============================] - 58s 95ms/step - loss: 2.9793\n",
            "Epoch 3/15\n",
            "609/609 [==============================] - 58s 94ms/step - loss: 2.8034\n",
            "Epoch 4/15\n",
            "609/609 [==============================] - 58s 94ms/step - loss: 2.6717\n",
            "Epoch 5/15\n",
            "609/609 [==============================] - 58s 95ms/step - loss: 2.5611\n",
            "Epoch 6/15\n",
            "609/609 [==============================] - 58s 94ms/step - loss: 2.4623\n",
            "Epoch 7/15\n",
            "609/609 [==============================] - 58s 94ms/step - loss: 2.3722\n",
            "Epoch 8/15\n",
            "609/609 [==============================] - 58s 94ms/step - loss: 2.2866\n",
            "Epoch 9/15\n",
            "609/609 [==============================] - 58s 94ms/step - loss: 2.2067\n",
            "Epoch 10/15\n",
            "609/609 [==============================] - 58s 94ms/step - loss: 2.1315\n",
            "Epoch 11/15\n",
            "609/609 [==============================] - 58s 94ms/step - loss: 2.0608\n",
            "Epoch 12/15\n",
            "609/609 [==============================] - 58s 95ms/step - loss: 1.9934\n",
            "Epoch 13/15\n",
            "609/609 [==============================] - 58s 94ms/step - loss: 1.9307\n",
            "Epoch 14/15\n",
            "609/609 [==============================] - 58s 95ms/step - loss: 1.8698\n",
            "Epoch 15/15\n",
            "609/609 [==============================] - 58s 95ms/step - loss: 1.8124\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc77d6c67d0>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "에포크를 진행할수록 loss가 낮아지는것을 확인할수 있습니다."
      ],
      "metadata": {
        "id": "A4kdMzQ65MOq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step7. 모델 평가해보기"
      ],
      "metadata": {
        "id": "j7ZRuz4N3YpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "  \n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    while True:\n",
        "        \n",
        "        predict = model(test_tensor)  # 1. 입력받은 문장의 텐서를 입력합니다\n",
        "        \n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]  # 2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
        "        \n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)  # 3. 2에서 예측된 word index를 문장 뒤에 붙입니다 \n",
        "        \n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        " \n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated"
      ],
      "metadata": {
        "id": "XO5Q8XxXZKAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "U4zIzK5TZNdI",
        "outputId": "c7357c79-0131-4430-f061-4b18a9f27b46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> i love you <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> what are\", max_len=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "G1ZWxRZe0Rwq",
        "outputId": "d03eec91-9f27-424f-cc84-2811919bcb8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> what are you waiting for wow ? <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> i can\", max_len=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "1b_8vntj06sr",
        "outputId": "495c9ed6-bec3-4033-8c6c-7f8541406b42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> i can t help it if i wanted to <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "사람이 직접봤을때 완벽하진않지만, 충분히 이해할 수 있는 문장이 만들어졌습니다."
      ],
      "metadata": {
        "id": "nIisnbYs5X61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 회고\n",
        "NLP는 처음다뤄봤기때문에 낯선개념들이 많이 등장해서 코드하나하나 상세하게 이해하지는 못했다..텐서부터 토근화, 각종 계층들 전부 새롭게 배운 개념들이다. 그리고 전처리과정에서 정규식을 사용해서 특정패턴의 문자열들을 변환시켜줬는데, 종종 정규식을 마주쳐왔었지만 상세코드는 이해하지않고 그냥넘어갔었다. 하지만 NLP를 하려면 정규식을 많이 공부해야한다는것을 느꼈다. 토큰화든 뭐든간에 반드시 전처리와 정제과정을 거쳐야하기때문이다. 그리고 너무 긴문장들은 가사에서 사용하기에 어울리지않기때문에 토큰이 15개 넘어가는 문장은 삭제를해주라는 조건이있었다. 간단한 조건이었지만, 부족한 실력탓에 꽤 시간을 많이썼다.. 행을 삭제하는 과정에서 인덱스에러가 계속 뜨길래 이해가안됬었다. 그래서 작은 단위로 계속 출력해서보니 행을 삭제하면 전체 shape의 행숫자가 바뀌는것이었다. 이거를 고려하지않아서 for반복문을 통해서 행을 지우다보니까 처음전체행보다 더 높은숫자의 행을 지우려해서 인덱스에러가뜬것이었다..알고리즘 공부도 꾸준히해야함을 느꼈다.낯선개념들이 많이 나와서 새롭게 공부해야할 양이 많았지만, 하나하나의 개념이 전부 이해못할정도로 어려운건아니었고, 마지막에 모델을 학습시키고 문장을 새로 뽑아줄때 나름의 재미와 신기함을 느꼈다. "
      ],
      "metadata": {
        "id": "FwrplLkA5eLG"
      }
    }
  ]
}